# 13_强化学习

## 马尔可夫链（Markov chain）和马尔可夫决策过程

### 马尔可夫链

马尔可夫链（Markov chain），又称离散时间马可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马可夫性质。马尔科夫链作为实际过程的统计模型具有许多应用。

对于一个一般的随机过程，其下一时刻的状态分布可能是之前所有时刻历经的状态的函数：

$$
\mathcal{P}_{ss'} = \mathbb{P} [ S_{t+1} = s_{t+1}' | S_t = s_t , S_{t-1} = s_{t-1}, ...]
$$

而对于马尔可夫过程，其下一时刻的状态分布仅与当前状态有关，与再之前的状态无关：

$$
\mathcal{P}_{ss'} = \mathbb{P} [ S_{t+1} = s_{t+1}' | S_t = s_t]
$$

在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变叫做转移，与不同的状态改变相关的概率叫做转移概率。随机漫步就是马尔可夫链的例子。随机漫步中每一步的状态是在图形中的点，每一步可以移动到任何一个相邻的点，在这里移动到每一个点的概率都是相同的（无论之前漫步路径是如何的）。

下图所示的例子为一个只有两个状态的马尔可夫链。当处于状态 $E$ 的时候，下一时刻，有 $70%$ 的概率转变为状态 $A$，而有 $30%$ 的概率保持状态 $E$ 不变；当处于状态 $A$ 的时候，下一时刻，有 $40%$ 的概率转变为状态 $E$，而有 $60%$ 的概率保持状态 $A$ 不变。

<img src="ml/markov.png" width="300" style="background: white"/>

我们也可以使用矩阵来表示状态转移的概率：

$$
\mathcal{P} = \begin{bmatrix} [\mathcal{P}_{11} & \hdots & \mathcal{P}_{1n} \\ \vdots & \hdots & \vdots \\ \mathcal{P}_{n1} & \hdots & \mathcal{P}_{nn} ] \end{bmatrix}
$$

其中 $\mathcal{P}_ij$ 代表状态为 $S_i$ 时，转移到 $S_j$ 的概率。显然应有 $ \forall i,  \sum_{k=1}^n \mathcal{P}_{ik} = 1$。

我们可以将一个马尔可夫过程记为 $<S, \mathcal{P}>$，其中 $S$ 为包含所有可能状态的集合，$\mathcal{P}$ 为状态转移矩阵。
 
### 马尔可夫奖励过程

马尔可夫奖励过程在马尔可夫过程的基础上引入了奖励 $R$ 和奖励衰减系数 $\gamma$： $<S, \mathcal{P}, R, \gamma>$。

$S$ 状态下的奖励定义为， 某一时刻处在状态 $s$ 下，下一个时刻能获得的奖励的期望：

$R_s = E[R_{t+1} | S_t = s]$

这里要注意，以离开该状态时的奖励的期望作为奖励仅仅是为方便起见进行的约定。同样我们也可以约定以进入该状态时的奖励作为该时刻的奖励，也是可以的。

我们将一个马尔可夫奖励过程中，时刻$t$之后获得的所有的奖励的有衰减的和定义为收益（gain, 也可称为收获或回报）：

$$
G_t = R_{t+1} + \gamma R_{t+2} + \hdots = \sum_{k=0}^{\infty} R_{t+k+1}
$$

这里我们常使用 $\gamma \in (0, 1]$ 作为衰减系数。直观上来说我们会选择一个小于$1$的衰减系数，可以认为我们并不平等地取之后所有奖励之和，而更青睐于短期内可以获得的奖励。这是一种相对简洁的数学表达。

接下来我们可以定义价值函数。一个马尔可夫奖励过程中，某一状态的价值函数定义为，从该状态开始的马尔可夫链的收益的数学期望：

$$
v(s) = E[G_t | S_t = s]
$$

价值函数给出了某一状态的长期价值。

到这里，我们有了马尔可夫过程这一模型，也有了一套评价各个状态好坏的价值函数。但是马尔可夫过程本身是随机的，我们并没有办法控制系统的状态，因此对于各个状态的好坏的评价意义也不大。下面我们来引入行动。

### 马尔可夫决策过程（Markov Decision Process, MDP）

相较于马尔可夫奖励过程，马尔可夫决策过程增加了每个状态下的行动。由一个状态转移为另一个状态并不再是完全随机的，而是由当前状态和当前状态下采取的行动决定的。
