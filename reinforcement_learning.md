# 13_强化学习

## 马尔可夫链（Markov chain）和马尔可夫决策过程

### 马尔可夫链

马尔可夫链（Markov chain），又称离散时间马可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马可夫性质。马尔科夫链作为实际过程的统计模型具有许多应用。

对于一个一般的随机过程，其下一时刻的状态分布可能是之前所有时刻历经的状态的函数：

$$
\mathcal{P}_{ss'} = \mathbb{P} [ S_{t+1} = s_{t+1}' | S_t = s_t , S_{t-1} = s_{t-1}, ...]
$$

而对于马尔可夫过程，其下一时刻的状态分布仅与当前状态有关，与再之前的状态无关：

$$
\mathcal{P}_{ss'} = \mathbb{P} [ S_{t+1} = s_{t+1}' | S_t = s_t]
$$

在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变叫做转移，与不同的状态改变相关的概率叫做转移概率。随机漫步就是马尔可夫链的例子。随机漫步中每一步的状态是在图形中的点，每一步可以移动到任何一个相邻的点，在这里移动到每一个点的概率都是相同的（无论之前漫步路径是如何的）。

下图所示的例子为一个只有两个状态的马尔可夫链。当处于状态 $E$ 的时候，下一时刻，有 $70%$ 的概率转变为状态 $A$，而有 $30%$ 的概率保持状态 $E$ 不变；当处于状态 $A$ 的时候，下一时刻，有 $40%$ 的概率转变为状态 $E$，而有 $60%$ 的概率保持状态 $A$ 不变。

<img src="ml/markov.png" width="300" style="background: white"/>

### 马尔可夫奖励过程




相较于马尔可夫链，马尔可夫决策过程增加了每个状态下的行动。由一个状态转移为另一个状态并不再是完全随机的，而是由当前状态和当前状态下采取的行动决定的。
